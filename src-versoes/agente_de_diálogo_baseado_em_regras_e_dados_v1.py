# -*- coding: utf-8 -*-
"""Agente de diálogo baseado em regras e dados.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fWfQot8B3XeHJ21ZTs7CgLd9C9XYkLRb

># AGENTE DE DIÁLOGO HÍBRIDO BASEADO EM REGRAS E DADOS
>CURSO: Tecnólogo em Inteligência Artificial Aplicada  
>DISCIPLINA: Agentes Conversacionais  
>AUTORA: Carla Edila Silveira  
>OBJETIVO: construir um agente de diálogo que trará ocorrências sobre determinado tema  
> DATA: 05/09/2023
______________________________________________________________________

<body>
<center>
<img src="https://i.postimg.cc/0Q7ZcBm7/header.png" align="middle">
</center>
</body>

>## 1. Qual o contexto do projeto?
><p align="justify">Um agente de diálogo de <i>question answering</i> que, baseado em um corpus de texto sobre um assunto, traz informações mais relevantes de acordo com a consulta do usuário.</p>

>## 2. Quais ferramentas e técnicas adotar?
>*   **NLTK** - toolkit de PLN em Python
>*   **Expressões Regulares** - pacote de regex do Python
>*   **urllib e BeautifulSoup** - bibliotecas para obter dados de páginas HTML
>*   **scikit-learn** - pacote com funcionalidades de manipulação de dados e Machine Learning (serão utilizados TF-IDF e Similaridade de cosseno)

> ## 3. Construção do agente de diálogo
>A operação do agente será deste modo:
>1.   Recebe **entrada** do usuário
>2.   **Pré-processa** a entrada do usuário
>3.   Calcula a **similaridade** entre a entrada e as sentenças do corpus
>4.   Obtém a sentença **mais similar do corpus**
>5.   Mostra-a como **resposta** ao usuário  

>Antes destas etapas, será criado o corpus ao obter automaticamente dados da Wikipedia.

> ## 4. Importação de bibliotecas
> Importar pacote de expressões regulares do Python e acesso ao WordNet dado pelo NLTK.
"""

import nltk
nltk.download('punkt')
nltk.download('rslp')# Stemming em pt-br
from nltk.corpus import stopwords
nltk.download('stopwords')# Lista de stopwords
import numpy as np
import random
import string
import bs4 as bs
import urllib.request
import re
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)

""">## 5. Construção do corpus
><p align="justify">Será feito um <i>web-scraping</i> para obter os dados automaticamente da Wikipedia. Este processo deve ser executado só uma vez, e o arquivo salvo em forma de texto na máquina.</p>
"""

# Busca pagina sobre o BRASIL. Para mudar o tema basta colocar o link para outra pagina.
# É possivel obter dados de páginas diferentes, basta definir uma lista de links e iterar sobre elas.
codigo_html = urllib.request.urlopen('https://pt.wikipedia.org/wiki/Brasil')
codigo_html = codigo_html.read()

# Processa codigo HTML lido
html_processado = bs.BeautifulSoup(codigo_html, 'lxml')

# Busca todos os paragrafos do texto
paragrafos = html_processado.find_all('p')

texto = ''

# Percorre paragrafos e concatena textos
for p in paragrafos:
  texto += p.text

# Normaliza texto para minusculas
texto = texto.lower()
texto[0:1000]

""">## 6. Pré-processamento do corpus
> É necessário remover caracteres especiais do texto e dividí-lo em sentenças válidas.
"""

texto = re.sub(r'\[[0-9]*\]', ' ', texto)
texto = re.sub(r'\s+', ' ', texto)
texto[0:1000]

sentencas = nltk.sent_tokenize(texto, language='portuguese')
palavras = nltk.word_tokenize(texto, language='portuguese')
sentencas[10:15]

""">## 7. Funções de pré-processamento de entrada do usuário
><p align="justify">Cria funções para pré-processar as entradas do usuário, com retirada de  pontuações e uso de Stemming nos textos, para que palavras similares sejam processadas igualmente pelo algoritmo (por ex., pedra e pedregulho teriam mesma forma léxica).</p>
"""

# Define funcao que faz Stemming em todo texto
def stemming(tokens):
  stemmer = nltk.stem.RSLPStemmer()
  novotexto = []
  for token in tokens:
    novotexto.append(stemmer.stem(token.lower()))
  return novotexto

# Funcao que remove pontuacao
removePontuacao = dict((ord(punctuation), None) for punctuation in string.punctuation)

def preprocessa(documento):
  return stemming(nltk.word_tokenize(documento.lower().translate(removePontuacao), language='portuguese'))

# Conferir como fica um texto apos seu pré-processamento
preprocessa("Olá meu nome é Lucas, eu moro no Brasil, e você?")

""">## 8. Resposta a saudações
><p align="justify">Embora seja um sistema de diálogo baseado em tarefas, é provável que o usuário inicie conversas com saudações ao agente. Por isso, será desenvolvida uma função (regras) para tratar desta situação.</p>  
><p align="justify">Serão criadas algumas respostas possíveis, dentre as quais serão escolhidas algumas aleatoriamente, para evitar que o agente fique repetitivo.</p>
"""

saudacoes_entrada = ("olá", "bom dia", "boa tarde", "boa noite", "oi", "como vai", "e aí", "tudo bem")
saudacoes_respostas = ["olá", "olá, espero que esteja tudo bem contigo", "oi", "Oie", "Seja bem-vindo, em que posso te ajudar?"]

def geradorsaudacoes(saudacao):
  for token in saudacao.split():
    if token.lower() in saudacoes_entrada:
      return random.choice(saudacoes_respostas)

# Ao executar este exemplo várias vezes serão vistas respostas diferenres
geradorsaudacoes('Olá')

""">## 9. Resposta a consultas do usuário
><p align="justify">Função para tratar consultas do usuário, com a comparação da similaridade entre a entrada do usuário e as sentenças do corpus. Se houver, a sentença mais similar será mostrada como resposta.</p>
><p align="justify">Nesta função utilizam-se os algoritmos TD-IDF (<i>Term Frequency – Inverse Document Frequency</i>) e similaridade de cosseno.</p>
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def geradorrespostas(entradausuario):
  resposta = ''
  sentencas.append(entradausuario)

  word_vectorizer = TfidfVectorizer(tokenizer=preprocessa, stop_words=stopwords.words('portuguese'))
  all_word_vectors = word_vectorizer.fit_transform(sentencas)
  similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)
  similar_sentence_number = similar_vector_values.argsort()[0][-2]

  matched_vector = similar_vector_values.flatten()
  matched_vector.sort()
  vector_matched = matched_vector[-2]

  if vector_matched == 0:
    resposta = resposta + "Me desculpe, não entendi o que você pediu."
    return resposta
  else:
    resposta = resposta + sentencas[similar_sentence_number]
    return resposta

""">## 10. Interação com o agente de diálogo
><p align="justify">Define um algoritmo que continue interagindo com o usuário até que ele decida finalizar.</p>  
><p align="justify">O resultado não é sempre o ideal, porém cobre muitas das possíveis perguntas. Se utilizássemos apenas regras de diálogo para responder perguntas sobre um assunto, precisaríamos de centenas de regras.</p>
><p align="justify"> Como as respostas são baseadas em dados, apenas uma regra que calcula similaridade com o corpus é suficiente. Faça perguntas como:</p>  

>*  *Qual o esporte mais popular no Brasil?*
>*  *Quais eventos esportivos o Brasil já organizou?*
>*  *Como é a cozinha brasileira?*
>*  *Onde são realizadas pesquisas tecnológicas no Brasil?*

"""

continue_dialogue = True
print("Olá, eu sou o Agente Tupiniquim. Me pergunte qualquer coisa sobre nosso país.")
while (continue_dialogue == True):
  # Obtem entrada do usuario
  human_text = input().lower()

  if human_text != 'tchau':
    if human_text == 'obrigado' or human_text == 'muito obrigado' or human_text == 'agradecido':
      continue_dialogue = False
      print("Agente Tupiniquim: Disponha")
    else:
      if geradorsaudacoes(human_text) != None:
        print("Agente Tupiniquim: " + geradorsaudacoes(human_text))
      else:
        print("Agente Tupiniquim: ", end="")
        print(geradorrespostas(human_text))
        sentencas.remove(human_text)
  else:
    continue_dialogue = False
    print("Agente Tupiniquim: Até a próxima.")

""">## 11. Como melhorar o projeto?
><p align="justify">Este agente de diálogo utiliza modelo baseado em regras, em que uma das regras usa corpus de dados para formular respostas. Desse modo, o modelo ficou mais flexível, sem necessidade de criar centenas/milhares de regras. Vejamos como melhorar o projeto:</p>  
><p align="justify">A) Além dos parágrafos (tag "p") da página da Wikipedia, podem ser usados dados dispostos na coluna direita, que trazem informações relevantes, como população, atual presidente etc., para montar sentenças.</p>  
><p align="justify">B) Melhorar o cálculo de similaridade com uso de um modelo de Word Embeddings, além do TF-IDF.</p>
><p align="justify">C) Obter dados sobre o Brasil a partir de diferentes fontes.</p>
><p align="justify">D) Criar classificador de contexto para o agente e, de modo dinâmico, buscar páginas da Wikipedia correspondentes à pergunta do usuário, para depois dar a resposta. Desse modo o agente não se limitaria a perguntas sobre o Brasil.</p>

>## 12. Referências e material complementar  
>* [Python for NLP: Creating a Rule-Based Chatbot](https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/)
>* [Building a Simple Chatbot from Scratch in Python (using NLTK)](https://morioh.com/p/6cc33336784c)
>* [Building a simple chatbot in python](https://medium.com/nxtplus/building-a-simple-chatbot-in-python-3963618c490a)
>* [Designing A ChatBot Using Python: A Modified Approach](https://towardsdatascience.com/designing-a-chatbot-using-python-a-modified-approach-96f09fd89c6d)
>* [Build Your First Python Chatbot Project](https://dzone.com/articles/python-chatbot-project-build-your-first-python-pro)
>* [Python Chatbot Project – Learn to build your first chatbot using NLTK & Keras](https://data-flair.training/blogs/python-chatbot-project/)
>* [Python Chat Bot Tutorial - Chatbot with Deep Learning (Part 1)](https://www.youtube.com/watch?v=wypVcNIH6D4)
>* [Intelligent AI Chatbot in Python](https://www.youtube.com/watch?v=1lwddP0KUEg)
>* [Coding a Jarvis AI Using Python 3 For Beginners](https://www.youtube.com/watch?v=NZMTWBpLUa4)  
>* Projeto elaborado a partir de Notebook criado por Prof. [Lucas Oliveira](http://lattes.cnpq.br/3611246009892500).
"""