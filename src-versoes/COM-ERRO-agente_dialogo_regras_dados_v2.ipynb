{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idIMZgaR3IqM"
      },
      "source": [
        "># AGENTE DE DIÁLOGO HÍBRIDO BASEADO EM REGRAS E DADOS\n",
        ">CURSO: Tecnólogo em Inteligência Artificial Aplicada  \n",
        ">DISCIPLINA: Agentes Conversacionais  \n",
        ">AUTORA: Carla Edila Silveira  \n",
        ">OBJETIVO: construir um agente de diálogo que trará ocorrências sobre determinado tema  \n",
        ">MELHORIA:   \n",
        ">GITHUB: https://github.com/rosacarla/Chatbot-baseado-em-regras-e-dados  \n",
        ">DATA: 05/09/2023\n",
        "______________________________________________________________________\n",
        "\n",
        "<body>\n",
        "<center>\n",
        "<img src=\"https://i.postimg.cc/0Q7ZcBm7/header.png\" align=\"middle\">\n",
        "</center>\n",
        "</body>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDw6X5Dq3h_Z"
      },
      "source": [
        ">## 1. Qual o contexto do projeto?\n",
        "><p align=\"justify\">Um agente de diálogo de <i>question answering</i> que, baseado em um corpus de texto sobre um assunto, traz informações mais relevantes de acordo com a consulta do usuário.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XlqXcfuAPbQ"
      },
      "source": [
        ">## 2. Quais ferramentas e técnicas adotar?\n",
        ">*   **NLTK** - toolkit de PLN em Python\n",
        ">*   **Expressões Regulares** - pacote de regex do Python\n",
        ">*   **urllib e BeautifulSoup** - bibliotecas para obter dados de páginas HTML\n",
        ">*   **scikit-learn** - pacote com funcionalidades de manipulação de dados e Machine Learning (serão utilizados TF-IDF e Similaridade de cosseno)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHKrJBjdBqw1"
      },
      "source": [
        "> ## 3. Construção do agente de diálogo\n",
        ">A operação do agente será deste modo:\n",
        ">1.   Recebe **entrada** do usuário\n",
        ">2.   **Pré-processa** a entrada do usuário\n",
        ">3.   Calcula a **similaridade** entre a entrada e as sentenças do corpus\n",
        ">4.   Obtém a sentença **mais similar do corpus**\n",
        ">5.   Mostra-a como **resposta** ao usuário  \n",
        "\n",
        ">Antes destas etapas, será criado o corpus ao obter automaticamente dados da Wikipedia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb2Dp_OFJAyO"
      },
      "source": [
        "> ## 4. Importação de bibliotecas\n",
        "> Importar pacote de expressões regulares do Python e acesso ao WordNet dado pelo NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRDAwuHdJZoo",
        "outputId": "f5123121-42ae-45ba-dfec-2875856f11de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')# Stemming em pt-br\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')# Lista de stopwords\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iMVzxB8RB-F"
      },
      "source": [
        ">## 5. Construção do corpus\n",
        "><p align=\"justify\">Será feito um <i>web-scraping</i> para obter os dados automaticamente da Wikipedia. Este processo deve ser executado só uma vez, e o arquivo salvo em forma de texto na máquina.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UPAfE57RCIu",
        "outputId": "6c1bd9ae-fdea-4c1d-aad8-30b3c9db4b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto da página https://pt.wikipedia.org/wiki/Brasil:\n",
            "coordenadas: 10° s 55° o\n",
            "brasil (localmente [bɾaˈziw][b]), oficialmente república federativa do brasil (? escutar),[7] é o maior país da américa do sul e da região da américa latina, sendo o quinto maior do mundo em área territorial (equivalente a 47,3% do território sul-americano), com 8 510 417,771 km²,[1][8][9] e o sétimo em população[10][11] (com 203 milhões de habitantes, em agosto de 2022).[12] é o único país na américa onde se fala majoritariamente a língua portuguesa e o maior país lusófono do planeta,[13] além de ser uma das nações mais multiculturais e etnicamente diversas, em decorrência da forte imigração oriunda de variados locais do mundo. sua atual constituição, promulgada em 1988, concebe o brasil como uma república federativa presidencialista,[7] formada pela união dos 26 estados, do distrito federal e dos 5 570 municípios.[7][14][nota 1]\n",
            "\n",
            "banhado pelo oceano atlântico, o brasil tem um litoral de 7 491 km[13] e faz fronteira com todos os outros países sul-americanos, e\n",
            "\n",
            "Texto da página https://brasilescola.uol.com.br/geografia/pais-brasil.htm:\n",
            "o brasil é um país de dimensões continentais localizado na américa do sul, com capital em brasília. apresenta uma enorme diversidade paisagística, econômica e cultural.\n",
            "publicidade\n",
            "o brasil é o maior país da américa do sul, com área de mais de 8,5 milhões de km². tem como capital a cidade de brasília. apresenta uma grande variedade climática e paisagística, que pode ser analisada mediante os domínios morfoclimáticos.\n",
            "a população brasileira chegou a 213.317.639 habitantes em 2021, de acordo com o ibge, com mais de 87% dela vivendo nas cidades. são paulo, capital do estado de mesmo nome, constitui a maior área urbana do brasil, com 12 milhões de habitantes.\n",
            "a economia brasileira, embora liderada pelo setor terciário, se destaca também em áreas como a indústria petroquímica e automobilística e na produção agropecuária, que tem a soja como carro-chefe.\n",
            "leia também: quais são os estados brasileiros?\n",
            " tópicos deste artigo1 - resumo sobre o brasil2 - dados gerais do brasil3 - história do bras\n",
            "\n",
            "Texto da página https://www.brasilparalelo.com.br/artigos/historia-do-brasil:\n",
            "a histã³ria do brasil comeã§ou antes mesmo do paã­s receber esse nome. em 1500 os portugueses fizeram uma grande expediã§ã£o que oficialmente buscava encontrar uma rota comercial para a ãndia e propagar a fã© cristã£.\n",
            "no meio do caminho esses navios se desviaram da rota oficial e desembarcaram em terras atã© entã£o pouco conhecidas, onde jã¡ existiam povos nativos que eram totalmente estranhos aos europeus.\n",
            "e esse ã© o inã­cio da histã³ria do paã­s que futuramente recebeu o nome de brasil.â â \n",
            "professores renomados como alberto da costa e silva, percival puggina, marcus boeira, thomas giulliano e muitos outros foram reunidos para resgatarem a histã³ria do brasil. assista ao documentã¡rio brasil: a ãltima cruzada.\n",
            "na virada do sã©culo xv para o sã©culo xvi o ser humano se jogou em uma nova empreitada: a expansã£o marã­tima.\n",
            "as duas principais motivaã§ãµes para isso eram religiosas e econã´micas.\n",
            "cristã³vã£o colombo, o homem que descobriu a amã©rica, tinha como uma de suas grandes moti\n",
            "\n",
            "Texto da página https://www.todamateria.com.br/a-historia-do-brasil/:\n",
            "a história do brasil começou com a ocupação dos seres humanos a cerca de 12-20 mil anos.\n",
            "no século xvi, os portugueses começaram a colonizar estas terras e transferiram africanos para serem mão de obra escrava nos engenhos que construíram aqui.\n",
            "por sua vez, estes trabalhadores forçados trariam novos alimentos e animais que mudariam a história dos povos originários para sempre.\n",
            "há evidências da presença de humanos no brasil há pelo menos 12 mil anos. cerca de três grandes grupos de seres humanos primitivos ocuparam o brasil, como os caçadores-coletores, os sambaquis e os povos agricultores.\n",
            "podemos encontrar vestígios dos povos da pré-história em vários pontos do brasil como, por exemplo, na serra da capivara (pi) ou em lajedo de soledade (rs).\n",
            "em 1500, os portugueses se deram conta que há terras no sul da linha do equador e passaram a ocupar o território. isto mudaria para sempre a vida dos povos indígenas, dos africanos e dos europeus.\n",
            "de acordo com a história oficial do brasil, este \n",
            "\n",
            "Texto da página https://www.observatoriodasmetropoles.net.br/ibge-apresenta-os-primeiros-resultados-do-censo-2022/:\n",
            "de acordo com o instituto brasileiro de geografia e estatística (ibge), a população do brasil atingiu 203.062.512 pessoas, com um aumento de 12,3 milhões na última década – número abaixo da estimativa do órgão, que apontava para um total de 207,7 milhões de pessoas. os primeiros dados do censo demográfico 2022 foram divulgados na quarta-feira, dia 28, no auditório do museu do amanhã, localizado na cidade do rio de janeiro.\n",
            "a edição de 2022 começou a ser planejada em 2017, com a intenção de ser realizada em 2020. no entanto, devido à pandemia de covid-19, o processo foi interrompido e novos planos tiveram que ser traçados. ainda com incertezas, incluindo a questão orçamentária diante dos cortes no orçamento federal anunciados pelo governo bolsonaro, os preparativos só foram retomados em 2021. cimar azeredo pereira, presidente substituto do ibge, falou sobre os desafios enfrentados e mencionou a decisão de utilizar a ilha de paquetá (rj) como teste para o censo, uma vez que todos os habi\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Busca paginas sobre o BRASIL. Para mudar o tema basta colocar o link para outra pagina.\n",
        "# Lista de URLs para as consultas\n",
        "urls = [\n",
        "    'https://pt.wikipedia.org/wiki/Brasil',\n",
        "    'https://brasilescola.uol.com.br/geografia/pais-brasil.htm',\n",
        "    'https://www.brasilparalelo.com.br/artigos/historia-do-brasil',\n",
        "    'https://www.todamateria.com.br/a-historia-do-brasil/',\n",
        "    'https://www.observatoriodasmetropoles.net.br/ibge-apresenta-os-primeiros-resultados-do-censo-2022/'\n",
        "]\n",
        "\n",
        "corpus = []  # Inicializa lista para armazenar o texto\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        # Faz solicitacao HTTP para obter conteudo da pagina\n",
        "        codigo_html = requests.get(url)\n",
        "\n",
        "        # Verifica se solicitacao foi bem-sucedida (código de status 200)\n",
        "        if codigo_html.status_code == 200:\n",
        "            # Processa o conteudo HTML da pagina\n",
        "            html_processado = BeautifulSoup(codigo_html.text, 'html.parser')\n",
        "\n",
        "            # Encontra todos os paragrafos do texto\n",
        "            paragrafos = html_processado.find_all('p')\n",
        "\n",
        "            # Concatena textos de paragrafos em unica string\n",
        "            texto = '\\n'.join([p.get_text() for p in paragrafos])\n",
        "\n",
        "            # Normaliza texto para minusculas\n",
        "            texto = texto.lower()\n",
        "\n",
        "            # Adiciona texto ao corpus\n",
        "            corpus.append(texto)\n",
        "\n",
        "            # Imprime primeiros 1000 caracteres do texto\n",
        "            print(f'Texto da página {url}:\\n{texto[:1000]}\\n')\n",
        "        else:\n",
        "            print(f'Falha ao acessar a página {url}: {codigo_html.status_code}')\n",
        "    except Exception as e:\n",
        "        print(f'Erro ao processar a página {url}: {str(e)}')\n",
        "\n",
        "# Salva corpus em um arquivo de texto\n",
        "with open('corpus.txt', 'w', encoding='utf-8') as arquivo:\n",
        "    for texto in corpus:\n",
        "        arquivo.write(texto + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjAXpxbwT5IB"
      },
      "source": [
        ">## 6. Pré-processamento do corpus\n",
        "> É necessário remover caracteres especiais do texto e dividí-lo em sentenças válidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "XS7MKMOCT5XM",
        "outputId": "a97ae6b4-d39b-49d6-f6b0-8c984bd3739f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'de acordo com o instituto brasileiro de geografia e estatística (ibge), a população do brasil atingiu 203.062.512 pessoas, com um aumento de 12,3 milhões na última década – número abaixo da estimativa do órgão, que apontava para um total de 207,7 milhões de pessoas. os primeiros dados do censo demográfico 2022 foram divulgados na quarta-feira, dia 28, no auditório do museu do amanhã, localizado na cidade do rio de janeiro. a edição de 2022 começou a ser planejada em 2017, com a intenção de ser realizada em 2020. no entanto, devido à pandemia de covid-19, o processo foi interrompido e novos planos tiveram que ser traçados. ainda com incertezas, incluindo a questão orçamentária diante dos cortes no orçamento federal anunciados pelo governo bolsonaro, os preparativos só foram retomados em 2021. cimar azeredo pereira, presidente substituto do ibge, falou sobre os desafios enfrentados e mencionou a decisão de utilizar a ilha de paquetá (rj) como teste para o censo, uma vez que todos os habi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "texto = re.sub(r'\\[[0-9]*\\]', ' ', texto)\n",
        "texto = re.sub(r'\\s+', ' ', texto)\n",
        "texto[0:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FGlLHzSUU7N",
        "outputId": "d107d671-51c6-4e0f-f426-95a13ee8ef03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['os dispositivos móveis de coleta (dmcs) utilizaram sistemas georreferenciados, chips 4g e wifi, o que tornou a gestão da coleta de dados mais ágil.',\n",
              " 'agora, esses dispositivos serão utilizados pelas secretarias de saúde em todo o brasil.',\n",
              " 'os dados preliminares mostram que a população do brasil atingiu 203.062.512 pessoas.',\n",
              " 'o censo 2022 registrou um aumento de 12,3 milhões desde 2010, o que representa um crescimento médio da população de 0,52% nos últimos anos.',\n",
              " 'o país agora tem 90,7 milhões de domicílios, 34% a mais do que há uma década.']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "sentencas = nltk.sent_tokenize(texto, language='portuguese')\n",
        "palavras = nltk.word_tokenize(texto, language='portuguese')\n",
        "sentencas[10:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65nR3LBTVmy0"
      },
      "source": [
        ">## 7. Funções de pré-processamento de entrada do usuário\n",
        "><p align=\"justify\">Cria funções para pré-processar as entradas do usuário, com retirada de  pontuações e uso de Stemming nos textos, para que palavras similares sejam processadas igualmente pelo algoritmo (por ex., pedra e pedregulho teriam mesma forma léxica).</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "FjMnCjqfVm6N"
      },
      "outputs": [],
      "source": [
        "# Define funcao que faz Stemming em todo texto\n",
        "def stemming(tokens):\n",
        "  stemmer = nltk.stem.RSLPStemmer()\n",
        "  novotexto = []\n",
        "  for token in tokens:\n",
        "    novotexto.append(stemmer.stem(token.lower()))\n",
        "  return novotexto\n",
        "\n",
        "# Funcao que remove pontuacao, stopwords e aplica stemming\n",
        "def preprocessa(documento):\n",
        "  # Remove pontuacao\n",
        "  documento = documento.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # Tokenização de palavras\n",
        "  tokens = nltk.word_tokenize(documento, language='portuguese')\n",
        "\n",
        "  # Remove stopwords\n",
        "  stopwords_pt = set(stopwords.words('portuguese'))\n",
        "  tokens = [token.lower() for token in tokens if token.lower() not in stopwords_pt]\n",
        "  removePontuacao = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "  # Aplica stemming\n",
        "  tokens = stemming(tokens)\n",
        "\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UysIjiIPW6nT",
        "outputId": "2db08cc4-ff4c-4039-cdeb-b96d2b0fd95f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['olá', 'nom', 'luc', 'mor', 'brasil']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "# Conferir como fica um texto apos seu pre processamento\n",
        "preprocessa(\"Olá meu nome é Lucas, eu moro no Brasil, e você?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUL4YvxMYMVc"
      },
      "source": [
        ">## 8. Resposta a saudações\n",
        "><p align=\"justify\">Embora seja um sistema de diálogo baseado em tarefas, é provável que o usuário inicie conversas com saudações ao agente. Por isso, será desenvolvida uma função (regras) para tratar desta situação.</p>  \n",
        "><p align=\"justify\">Serão criadas algumas respostas possíveis, dentre as quais serão escolhidas algumas aleatoriamente, para evitar que o agente fique repetitivo.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "WJNLddZgYMeg"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "saudacoes_entrada = (\"olá\", \"bom dia\", \"boa tarde\", \"boa noite\", \"oi\", \"como vai\", \"e aí\", \"tudo bem\")\n",
        "saudacoes_respostas = [\"olá\", \"olá, espero que esteja tudo bem contigo\", \"Olá! Como posso ajudar você hoje?\", \"oi\", \"Oie\",\n",
        "                       \"Seja bem-vindo, em que posso te ajudar?\", \"Oi! Estou à disposição para responder às suas perguntas.\",\n",
        "                       \"Olá! Estou aqui para fornecer informações sobre o Brasil. O que você gostaria de saber?\",\n",
        "                       \"Oi! Bem-vindo! O que você deseja saber sobre o Brasil?\"]\n",
        "\n",
        "# Funcao para responder a saudacao\n",
        "def geradorsaudacoes(saudacao):\n",
        "    for token in saudacao.split():\n",
        "        if token.lower() in saudacoes_entrada:\n",
        "            return random.choice(saudacoes_respostas)\n",
        "\n",
        "    # Se nenhuma saudacao conhecida for encontrada, retorna None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XzW0X0iLZjBU",
        "outputId": "24e24c18-e460-469d-fde8-8282bacebf1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Oi! Bem-vindo! O que você deseja saber sobre o Brasil?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "# Ao executar este exemplo várias vezes serao vistas respostas diferentes\n",
        "geradorsaudacoes('Olá')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_gdo4tjZ3X4"
      },
      "source": [
        ">## 9. Resposta a consultas do usuário\n",
        "><p align=\"justify\">Função para tratar consultas do usuário, com a comparação da similaridade entre a entrada do usuário e as sentenças do corpus. Se houver, a sentença mais similar será mostrada como resposta.</p>\n",
        "><p align=\"justify\">Nesta função utilizam-se os algoritmos TD-IDF (<i>Term Frequency – Inverse Document Frequency</i>) e similaridade de cosseno.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "zITJfxB0Z3ee"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "DczIGmyUaPMr"
      },
      "outputs": [],
      "source": [
        "# Funcao para responder a consultas do usuario\n",
        "def geradorrespostas(entradausuario, corpus):\n",
        "  resposta = ''\n",
        "\n",
        "  # Verifica se entrada do usuario eh string\n",
        "  if isinstance(entradausuario, str):\n",
        "\n",
        "   # Pre rocessamento da entrada do usuario\n",
        "    entrada_preprocessada = preprocessa(entradausuario)\n",
        "\n",
        "    # Pre processamento do corpus por sentencas\n",
        "    corpus_preprocessado = [preprocessa(sentenca) for sentenca in corpus]\n",
        "\n",
        "    # Cria vetorizador TF-IDF\n",
        "    vectorizer = TfidfVectorizer(tokenizer=preprocessa, stop_words=stopwords.words('portuguese'))\n",
        "\n",
        "    # Calcula os vetores TF-IDF para o corpus\n",
        "    tfidf_corpus = vectorizer.fit_transform(corpus_preprocessado)\n",
        "\n",
        "    # Calcula similaridade de cosseno entre vetor da entrada do usuário e vetores TF-IDF do corpus\n",
        "    similaridades = cosine_similarity(vectorizer.transform([entrada_preprocessada]), tfidf_corpus)\n",
        "\n",
        "    # Encontra indice da sentenca mais similar\n",
        "    indice_mais_similar = np.argmax(similaridades)\n",
        "\n",
        "    # Calcula similaridade maxima\n",
        "    similaridade_maxima = similaridades[0, indice_mais_similar]\n",
        "\n",
        "    if similaridade_maxima == 0:\n",
        "      resposta = resposta + \"Me desculpe, não entendi o que você pediu.\"\n",
        "    else:\n",
        "      resposta = resposta + corpus[indice_mais_similar]\n",
        "\n",
        "    return resposta\n",
        "\n",
        "  #sentencas.append(entradausuario)\n",
        "\n",
        "  #word_vectorizer = TfidfVectorizer(tokenizer=preprocessa, stop_words=stopwords.words('portuguese'))\n",
        "  #all_word_vectors = word_vectorizer.fit_transform(sentencas)\n",
        "  #similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "  #similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
        "\n",
        "  #matched_vector = similar_vector_values.flatten()\n",
        "  #matched_vector.sort()\n",
        "  #vector_matched = matched_vector[-2]\n",
        "\n",
        "  #if vector_matched == 0:\n",
        "    #resposta = resposta + \"Me desculpe, não entendi o que você pediu.\"\n",
        "    #return resposta\n",
        "  #else:\n",
        "    #resposta = resposta + sentencas[similar_sentence_number]\n",
        "    #return resposta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VQILIt-bgJ8"
      },
      "source": [
        ">## 10. Interação com o agente de diálogo\n",
        "><p align=\"justify\">Define um algoritmo que continue interagindo com o usuário até que ele decida finalizar.</p>  \n",
        "><p align=\"justify\">O resultado não é sempre o ideal, porém cobre muitas das possíveis perguntas. Se utilizássemos apenas regras de diálogo para responder perguntas sobre um assunto, precisaríamos de centenas de regras.</p>\n",
        "><p align=\"justify\"> Como as respostas são baseadas em dados, apenas uma regra que calcula similaridade com o corpus é suficiente. Faça perguntas como:</p>  \n",
        "\n",
        ">*  *Qual o esporte mais popular no Brasil?*\n",
        ">*  *Quais eventos esportivos o Brasil já organizou?*\n",
        ">*  *Como é a cozinha brasileira?*\n",
        ">*  *Onde são realizadas pesquisas tecnológicas no Brasil?*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "VMIT3KygbgTe",
        "outputId": "d166bf37-80e1-440e-88f7-1c4109eb1844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Olá, eu sou o Agente Tupiniquim. Me pergunte qualquer coisa sobre nosso país ou diga 'tchau' para sair.\n",
            "oi\n",
            "Agente Tupiniquim: Oi! Bem-vindo! O que você deseja saber sobre o Brasil?\n",
            "qual o esporte mais popular no Brasil?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-3e42a3f860ff>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Agente Tupiniquim: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgeradorsaudacoes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mresposta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeradorrespostas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresposta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Agente Tupiniquim: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresposta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-071047420d27>\u001b[0m in \u001b[0;36mgeradorrespostas\u001b[0;34m(entradausuario, corpus)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Calcula os vetores TF-IDF para o corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtfidf_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_preprocessado\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Calcula similaridade de cosseno entre vetor da entrada do usuário e vetores TF-IDF do corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ],
      "source": [
        "# Inicia interacao com usuario\n",
        "print(\"Olá, eu sou o Agente Tupiniquim. Me pergunte qualquer coisa sobre nosso país ou diga 'tchau' para sair.\")\n",
        "continue_dialogue = True\n",
        "while continue_dialogue:\n",
        "  # Obtem entrada do usuario\n",
        "  human_text = input().lower()\n",
        "\n",
        "  if human_text != 'tchau':\n",
        "    if human_text in ['obrigado', 'muito obrigado', 'agradecido', 'valeu', 'agradeço', 'beleza']:\n",
        "      continue_dialogue = False\n",
        "      print(\"Agente Tupiniquim: Disponha!\")\n",
        "    else:\n",
        "      if geradorsaudacoes(human_text) != None:\n",
        "        print(\"Agente Tupiniquim: \" + geradorsaudacoes(human_text))\n",
        "      else:\n",
        "        resposta = geradorrespostas(human_text, corpus)\n",
        "        if resposta:\n",
        "          print(\"Agente Tupiniquim: \", resposta)\n",
        "        else:\n",
        "          print(\"Agente Tupiniquim: Me desculpe, não entendi o que você pediu.\")\n",
        "  else:\n",
        "    continue_dialogue = False\n",
        "    print(\"Agente Tupiniquim: Até a próxima!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JMOce1PXp1h"
      },
      "source": [
        ">## 11. Como melhorar o projeto?\n",
        "><p align=\"justify\">Este agente de diálogo utiliza modelo baseado em regras, em que uma das regras usa corpus de dados para formular respostas. Desse modo, o modelo ficou mais flexível, sem necessidade de criar centenas/milhares de regras. Vejamos como melhorar o projeto:</p>  \n",
        "><p align=\"justify\">A) Além dos parágrafos (tag \"p\") da página da Wikipedia, podem ser usados dados dispostos na coluna direita, que trazem informações relevantes, como população, atual presidente etc., para montar sentenças.</p>  \n",
        "><p align=\"justify\">B) Melhorar o cálculo de similaridade com uso de um modelo de Word Embeddings, além do TF-IDF.</p>\n",
        "><p align=\"justify\">C) Obter dados sobre o Brasil a partir de diferentes fontes.</p>\n",
        "><p align=\"justify\">D) Criar classificador de contexto para o agente e, de modo dinâmico, buscar páginas da Wikipedia correspondentes à pergunta do usuário, para depois dar a resposta. Desse modo o agente não se limitaria a perguntas sobre o Brasil.</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP59ECc53nqF"
      },
      "source": [
        ">## 12. Referências e material complementar  \n",
        ">* [Python for NLP: Creating a Rule-Based Chatbot](https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/)\n",
        ">* [Building a Simple Chatbot from Scratch in Python (using NLTK)](https://morioh.com/p/6cc33336784c)\n",
        ">* [Building a simple chatbot in python](https://medium.com/nxtplus/building-a-simple-chatbot-in-python-3963618c490a)\n",
        ">* [Designing A ChatBot Using Python: A Modified Approach](https://towardsdatascience.com/designing-a-chatbot-using-python-a-modified-approach-96f09fd89c6d)\n",
        ">* [Build Your First Python Chatbot Project](https://dzone.com/articles/python-chatbot-project-build-your-first-python-pro)\n",
        ">* [Python Chatbot Project – Learn to build your first chatbot using NLTK & Keras](https://data-flair.training/blogs/python-chatbot-project/)\n",
        ">* [Python Chat Bot Tutorial - Chatbot with Deep Learning (Part 1)](https://www.youtube.com/watch?v=wypVcNIH6D4)\n",
        ">* [Intelligent AI Chatbot in Python](https://www.youtube.com/watch?v=1lwddP0KUEg)\n",
        ">* [Coding a Jarvis AI Using Python 3 For Beginners](https://www.youtube.com/watch?v=NZMTWBpLUa4)  \n",
        ">* Projeto elaborado a partir de Notebook criado por Prof. [Lucas Oliveira](http://lattes.cnpq.br/3611246009892500)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}